{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size after dropping Null values is 99.5% of the original data size\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'framingham_heart_disease.csv')\n",
    "df = df[['male', 'age', 'BMI', 'heartRate', 'sysBP']]\n",
    "original_size = df.shape[0]\n",
    "df.dropna(how='any', inplace=True)\n",
    "no_null_size = df.shape[0]\n",
    "df.insert(loc=0, column='intercept', value=1)\n",
    "print(f'Data size after dropping Null values is'\n",
    " f' {no_null_size / original_size * 100:0.3}% of the original data size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(df, size):\n",
    "    np.random.seed(555)\n",
    "    flag = True\n",
    "    sample = None\n",
    "    while flag: #will take another sample if there is one sex only\n",
    "        sample = df.sample(size,ignore_index=True)\n",
    "        counts_gender = sample[['male', 'heartRate']].groupby('male').count()\n",
    "        male_count = counts_gender['sysBP'][1]\n",
    "        female_count = counts_gender['sysBP'][0]\n",
    "        if female_count != 0 and male_count != 0:\n",
    "            if abs(female_count - male_count) <= size * 0.1:\n",
    "                flag = False\n",
    "    return sample\n",
    "\n",
    "sample = generate_sample(df, 200)\n",
    "X_variables = ['intercept','age', 'BMI', 'heartRate']\n",
    "y_variable = 'sysBP'\n",
    "X = df[X_variables]\n",
    "y = df[y_variable]\n",
    "X_sample = sample[X_variables]\n",
    "y_sample = sample[y_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "z_alpha = stats.norm.ppf(0.975)\n",
    "#Q1.a\n",
    "def calculate_beta(X, y):\n",
    "    # Calculate MLE\n",
    "    C = X.T @ X\n",
    "    C_inv = np.linalg.inv(C)\n",
    "    C_inv_X = C_inv @ X.T\n",
    "    beta = C_inv_X @ y\n",
    "    return np.round(beta, 4).to_numpy()\n",
    "\n",
    "def beta_CI(X, y, beta_sample, x_variables):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    C = np.linalg.inv(X.T @ X)\n",
    "    e = y - np.dot(X,beta_sample) #residuals\n",
    "    res_var_estimate = (1 / (n - p)) * (e.T @ e) #sigma-hat squared\n",
    "    CI = []\n",
    "    for i, variable in enumerate(x_variables):\n",
    "        std_estimate = np.sqrt(res_var_estimate * (C[i][i])) #SE of Beta_i\n",
    "        CI.append(np.round([beta_sample[i] - z_alpha * std_estimate,\n",
    "                            beta_sample[i] + z_alpha * std_estimate], 4))\n",
    "    return CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta = [41.7462  0.9039  0.785   0.3429]\n",
      "CI for beta_0 is: [15.3282 68.1642]\n",
      "CI for beta_1 is: [0.6169 1.1909]\n",
      "CI for beta_2 is: [0.0966 1.4734]\n",
      "CI for beta_3 is: [0.1438 0.542 ]\n"
     ]
    }
   ],
   "source": [
    "#Q1.a\n",
    "beta_sample = calculate_beta(X_sample,y_sample)\n",
    "regular_CI = beta_CI(X_sample, y_sample, beta_sample,X_variables)\n",
    "print(f'Beta = {beta_sample}')\n",
    "for i in range(beta_sample.size):\n",
    "    print(f\"CI for beta_{i} is: {regular_CI[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal based CI for beta_0 is: [20.7928 62.6996]\n",
      "Normal based CI for beta_1 is: [0.6238 1.184 ]\n",
      "Normal based CI for beta_2 is: [0.007 1.563]\n",
      "Normal based CI for beta_3 is: [0.1655 0.5203]\n"
     ]
    }
   ],
   "source": [
    "#Q1.b\n",
    "B = 400\n",
    "bootstrap_beta_hist = []\n",
    "bootstrap_beta_var = []\n",
    "np.random.seed(555)\n",
    "for i in range(B):\n",
    "    bootstrap_data = sample.sample(frac=1,replace=True) # sampling 200 rows with returns\n",
    "    X_i = bootstrap_data[X_variables]\n",
    "    y_i = bootstrap_data[y_variable]\n",
    "    bootstrap_beta = calculate_beta(X_i,y_i)\n",
    "    bootstrap_beta_hist.append(bootstrap_beta)\n",
    "    bootstrap_beta_var.append(bootstrap_beta)\n",
    "beta_se = np.std(bootstrap_beta_var,axis=0)\n",
    "normal_CI = []\n",
    "for i in range(X_sample.shape[1]):\n",
    "    CI = np.round([beta_sample[i] - z_alpha * beta_se[i],\n",
    "                        beta_sample[i] + z_alpha * beta_se[i]], 4)\n",
    "    normal_CI.append(CI)\n",
    "    print(f\"Normal based CI for beta_{i} is: {CI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivotal CI for beta_0 is: [22.9332 63.2129]\n",
      "Pivotal CI for beta_1 is: [0.6028 1.1811]\n",
      "Pivotal CI for beta_2 is: [0.0458 1.5722]\n",
      "Pivotal CI for beta_3 is: [0.1641 0.4998]\n"
     ]
    }
   ],
   "source": [
    "#Q1.c\n",
    "bootstrap_beta_hist = np.array(bootstrap_beta_hist)\n",
    "pivotal_CI = []\n",
    "for i,(beta_est,beta_bootstrap) in enumerate(zip(beta_sample,bootstrap_beta_hist.T)):\n",
    "    beta_quantiles = np.quantile(beta_bootstrap,[0.025,0.975])\n",
    "    CI = np.round([2*beta_est - beta_quantiles[1], 2*beta_est - beta_quantiles[0]],4)\n",
    "    pivotal_CI.append(CI)\n",
    "    print(f\"Pivotal CI for beta_{i} is: {CI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivotal CI for beta_0 is: [20.2795 60.5592]\n",
      "Pivotal CI for beta_1 is: [0.6267 1.205 ]\n",
      "Pivotal CI for beta_2 is: [-0.0022  1.5242]\n",
      "Pivotal CI for beta_3 is: [0.186  0.5217]\n"
     ]
    }
   ],
   "source": [
    "#Q1.d\n",
    "quantile_CI = []\n",
    "for i,(beta_est,beta_bootstrap) in enumerate(zip(beta_sample,bootstrap_beta_hist.T)):\n",
    "    beta_quantiles = np.quantile(beta_bootstrap,[0.025,0.975])\n",
    "    CI = np.round([beta_quantiles[0], beta_quantiles[1]],4)\n",
    "    quantile_CI.append(CI)\n",
    "    print(f\"Pivotal CI for beta_{i} is: {CI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "regular_CI_lengths = [ci[1]-ci[0] for ci in regular_CI]\n",
    "normal_CI_lengths = [ci[1]-ci[0] for ci in normal_CI]\n",
    "pivotal_CI_lengths = [ci[1]-ci[0] for ci in pivotal_CI]\n",
    "quantile_CI_lengths = [ci[1]-ci[0] for ci in quantile_CI]\n",
    "data = list(zip(regular_CI_lengths,normal_CI_lengths,pivotal_CI_lengths,quantile_CI_lengths))\n",
    "CI_lengths_comparison = pd.DataFrame(data,index=['regular','normal','pivotal','quantile'],columns=['beta 0','beta 1', 'beta 2', 'beta 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI_contains(CI,type,actual_betas):\n",
    "    for i,ci in enumerate(CI):\n",
    "        decision = 'contains' if ci[0]<=actual_betas[i]<=ci[1] else \"doesn't contain\"\n",
    "        print(f'the {type} CI {list(ci)} {decision} beta_{i} = {actual_betas[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular CI\n",
      "the regular CI [15.3282, 68.1642] contains beta_0 = 26.1297\n",
      "the regular CI [0.6169, 1.1909] contains beta_1 = 0.9203\n",
      "the regular CI [0.0966, 1.4734] contains beta_2 = 1.4356\n",
      "the regular CI [0.1438, 0.542] contains beta_3 = 0.3102\n",
      "\n",
      "Normal CI\n",
      "the normal CI [20.7928, 62.6996] contains beta_0 = 26.1297\n",
      "the normal CI [0.6238, 1.184] contains beta_1 = 0.9203\n",
      "the normal CI [0.007, 1.563] contains beta_2 = 1.4356\n",
      "the normal CI [0.1655, 0.5203] contains beta_3 = 0.3102\n",
      "\n",
      "Pivotal CI\n",
      "the pivotal CI [22.9332, 63.2129] contains beta_0 = 26.1297\n",
      "the pivotal CI [0.6028, 1.1811] contains beta_1 = 0.9203\n",
      "the pivotal CI [0.0458, 1.5722] contains beta_2 = 1.4356\n",
      "the pivotal CI [0.1641, 0.4998] contains beta_3 = 0.3102\n",
      "\n",
      "Quantile CI\n",
      "the quantile CI [20.2795, 60.5592] contains beta_0 = 26.1297\n",
      "the quantile CI [0.6267, 1.205] contains beta_1 = 0.9203\n",
      "the quantile CI [-0.0022, 1.5242] contains beta_2 = 1.4356\n",
      "the quantile CI [0.186, 0.5217] contains beta_3 = 0.3102\n"
     ]
    }
   ],
   "source": [
    "actual_betas = calculate_beta(X,y)\n",
    "print(\"Regular CI\")\n",
    "CI_contains(regular_CI,'regular',actual_betas)\n",
    "print(\"\\nNormal CI\")\n",
    "CI_contains(normal_CI,'normal',actual_betas)\n",
    "print(\"\\nPivotal CI\")\n",
    "CI_contains(pivotal_CI,'pivotal',actual_betas)\n",
    "print(\"\\nQuantile CI\")\n",
    "CI_contains(quantile_CI,'quantile',actual_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 - new sample\n",
    "df_new = df.merge(sample, how='left', indicator=True)\n",
    "df_new = df_new[df_new['_merge'] == 'left_only']\n",
    "new_sample = generate_sample(df_new,100)\n",
    "new_X_sample = new_sample[X_variables]\n",
    "new_y_sample = new_sample[y_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.a - prediction\n",
    "new_beta_sample = calculate_beta(new_X_sample,new_y_sample)\n",
    "new_y_pred = np.matmul(new_X_sample, new_beta_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.b - CI for E[y_new|x_new]\n",
    "B = 400\n",
    "prediction_hist = []\n",
    "for i in range(B):\n",
    "    bootstrap_data = new_sample.sample(frac=1,replace=True) # sampling 200 rows with returns\n",
    "    X_i = bootstrap_data[X_variables]\n",
    "    y_i = bootstrap_data[y_variable]\n",
    "    bootstrap_beta = calculate_beta(X_i,y_i)\n",
    "    bootstrap_y_pred = np.matmul(X_i,bootstrap_beta)\n",
    "    prediction_hist.append(bootstrap_y_pred)\n",
    "\n",
    "prediction_hist = np.array(prediction_hist)\n",
    "pred_se = np.std(prediction_hist,axis=0)\n",
    "pred_mean = np.mean(prediction_hist,axis=0)\n",
    "normal_CI = []\n",
    "for i in range(prediction_hist.shape[1]):\n",
    "    CI = np.round([pred_mean[i] - z_alpha * pred_se[i],\n",
    "                        pred_mean[i] + z_alpha * pred_se[i]], 4)\n",
    "    normal_CI.append(CI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96% of the Bootstraped-CIs contain the predicted value of Ynew,\n",
      "which is 0.01 far from the desired confidence level of 95%\n"
     ]
    }
   ],
   "source": [
    "#Q3.c\n",
    "actual_confidence = np.mean([ int(ci[0]<=pred<=ci[1])\n",
    "                              for ci,pred in zip(normal_CI,new_y_pred)])\n",
    "print(f'{actual_confidence}% of the Bootstraped-CIs contain the predicted value of Ynew,\\n'\n",
    "      f'which is {np.abs(actual_confidence-0.95):.3} far from the desired confidence level of 95%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3.d<br>\n",
    "We will use the formula from Tirgul 5:<br>\n",
    "$CI(Y_{new}|x_{new})=\\hat{Y}_{new}\\ \\pm\\ Z_{\\frac{\\alpha}{2}}\\sqrt{\\hat{\\sigma^2_{\\epsilon}} \\cdot X_{new} \\cdot C \\cdot X_{new}^T\\ + \\hat{\\sigma^2_{\\epsilon}}}$<br>\n",
    "So, we will calculate $\\hat{\\sigma^2_{\\epsilon}}$ within every bootstrap sample(aka the noise variance) and add it to the variance of the prediction $Var(\\hat{Y}_{new})$.<br>\n",
    "Normal estimation is still valid (as we used it in the CI of $E[\\hat{Y}_{new}|\\hat{x}_{new}]$) because the \"noise\" is normally distributed too."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "!!!!!!! Part B !!!!!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#we've dropped 'education' feature (irrelevant for our questions)\n",
    "#we've dropped 'glucose' feature since it had too many NaN values\n",
    "#we've dropped all NaN containing samples and left with 4088 samples\n",
    "df = pd.read_csv(r'/Users/avishagnevo/Desktop/ex1_stats2/framingham_heart_disease.csv')\n",
    "df.drop(['glucose', 'education', 'prevalentHyp', 'prevalentStroke'], inplace=True, axis=1)\n",
    "df.dropna(how='any', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Distribution visualization  ####\n",
    "#does heart rate measurements effect on 10-year risk of coronary heart disease varies between sex\n",
    "sns.pairplot(data=df, x_vars=['heartRate'], y_vars=['TenYearCHD'], hue='male', kind='reg',\n",
    "             markers=['o', 'x'], height=2, aspect=3)\n",
    "plt.title(\"Linear Regression\\nheart rate effect on 10-year risk of coronary heart disease varies between sex\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flag=True\n",
    "while flag: #will take another sample if there is one sex only\n",
    "    sample = df.sample(200)\n",
    "    counts_gender = sample[['male', 'heartRate']].groupby('male').count()\n",
    "    counts_chances = sample[['TenYearCHD', 'heartRate']].groupby('TenYearCHD').count()\n",
    "    #taking care that there are females and males +\n",
    "    #more than 1/5 of the people that has 10-year risk of coronary heart disease\n",
    "    if counts_gender['heartRate'][0] != 0 and counts_gender['heartRate'][1] != 0 and counts_chances['heartRate'][1] > 40:\n",
    "        flag = False\n",
    "sample_var = sample[['male', 'heartRate']].groupby('male').var()\n",
    "sample_var1 = sample_var['heartRate'][0] #woman\n",
    "sample_var2 = sample_var['heartRate'][1] #man\n",
    "\n",
    "mean_gender = sample[['male', 'heartRate']].groupby('male').mean()\n",
    "mu1 = mean_gender['heartRate'][0] #woman\n",
    "mu2 = mean_gender['heartRate'][1] #man\n",
    "delta = mu1 - mu2\n",
    "\n",
    "n_gender = sample[['male', 'heartRate']].groupby('male').count()\n",
    "n1 = n_gender['heartRate'][0] #woman\n",
    "n2 = n_gender['heartRate'][1] #man"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.3.a:\n",
    "as the average being an estimator of the expected value-\n",
    "$\\hat{\\mu_1} = \\overline{X}_{woman}$\n",
    "$\\hat{\\mu_2} = \\overline{X}_{man}$\n",
    "Define $X = X_{woman} - X_{man}$,\n",
    "MLE estimator for the expected value is $\\delta = \\overline{X} = \\hat{E}(X) = \\hat{E}(X_{woman} - X_{man}) =  \\hat{E}(X_{woman}) - \\hat{E}(X_{man}) \\rightarrow$ so $\\hat{\\delta} = \\hat{\\mu_1} - \\hat{\\mu_2} = \\overline{X}_{woman} - \\overline{X}_{man}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.3.b:\n",
    "Define $S^2_p = \\frac{(n_{woman} - 1)S^2_{woman} + (n_{man} - 1)S^2_{man}}{n_{woman} + n_{man} -2}$\n",
    "\n",
    "CI for the expected value of $E(X) = E(X_{woman} - X_{man})$ is $[\\hat{\\delta} - t_{\\frac{\\alpha}{2}} S_p\\sqrt{\\frac{1}{n_{woman}} + \\frac{1}{n_{man}}}$ , $\\hat{\\delta} + t_{1- \\frac{\\alpha}{2}} S_p\\sqrt{\\frac{1}{n_{woman}} + \\frac{1}{n_{man}}}]$\n",
    "We'll conduct normality visual check, also we'll conduct F test to check variances equality: $$\n",
    "H0 : \\sigma_{woman} = \\sigma_{man} \\\\\n",
    "H1 : \\sigma_{woman} \\neq \\sigma_{man} \\\\\n",
    "T.S. : F = \\frac{S_{man}^2}{S_{woman}^2} \\\\\n",
    "RR : F > F_{(n_{man}-1 , n_{woman}-1),0.95}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#CI\n",
    "t_quantile = 1.96 #1-a\n",
    "Sp_2 = (((n1 - 1) * sample_var1) + ((n2 - 1) * sample_var2)) / (n1 + n2 - 2)\n",
    "t_CI = (delta - t_quantile*np.sqrt(Sp_2*(1/n1 + 1/n2)),\n",
    "             delta + t_quantile*np.sqrt(Sp_2*(1/n1 + 1/n2)))\n",
    "print(f'CI for the expected values difference is:\\n({t_CI[0]:.03} , {t_CI[1]:.03})')\n",
    "\n",
    "t_test_stat = delta / (np.sqrt(Sp_2*(1/n1 + 1/n2)))\n",
    "t_RR = np.absolute(t_test_stat) > t_quantile\n",
    "t_pvalue = 1-stat.t.cdf(7.39,df=4086)+stat.t.cdf(-7.39,df=4086)\n",
    "\n",
    "# normalization check using histograms\n",
    "sns.histplot(sample[sample['male']==0]['heartRate'], color=\"orange\", bins=25)\n",
    "sns.histplot(sample[sample['male']==1]['heartRate'], color=\"blue\", bins=25)\n",
    "plt.legend(['female', 'male'])\n",
    "plt.title(\"Heart rate distribution of male and female groups\")\n",
    "\n",
    "# F testing\n",
    "F = sample_var1/sample_var2\n",
    "F_quantile = stat.f.ppf(0.95, n1, n2)\n",
    "print('F test results:')\n",
    "print(f'F = {F:.03}, and as the 0.95 quantile of F distibution with ({n1},{n2}) df = {F_quantile:.03} we {\"reject the null hypothesis\" if F>F_quantile else \"do not reject the null hypothesis\"}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.3.c:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_gender_all = df[['male', 'heartRate']].groupby('male').mean()\n",
    "mu1_all = mean_gender_all['heartRate'][0] #woman\n",
    "mu2_all = mean_gender_all['heartRate'][1] #man\n",
    "delta_all = mu1_all - mu2_all\n",
    "print(f'{\"The samples CI includes the expected values difference based on the whole sample\" if (t_CI[0] < delta_all < t_CI[1]) else \"The samples CI does not include the expected values difference based on the whole sample\"} ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.3.d:\n",
    "Wald test :\n",
    "$$\n",
    "H0 : \\delta = 0 \\\\\n",
    "H1 : \\delta \\neq 0 \\\\\n",
    "T.S. : W = \\frac{\\hat{\\delta}-\\delta_0}{\\hat{se}} \\\\\n",
    "R.R. : |W| > z_{\\frac{\\alpha}{2}}\n",
    "$$\n",
    "Permutation test :\n",
    "$$\n",
    "H0 : F_{X_{woman}} = F_{X_{man}} \\\\\n",
    "H1 : F_{X_{women}} \\succ F_{X_{men}} \\\\\n",
    "T.S. : T_0 = \\overline{X}_{women} - \\overline{X}_{men} \\\\\n",
    "R.R. : \\frac{1}{N \\choose n_{woman}} \\sum_{i=1}^{N \\choose n_{woman}}I\\{T_i \\geq T_0\\} < \\alpha\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Wald test\n",
    "W = delta / np.sqrt(Sp_2)\n",
    "z_quantile = stat.norm.ppf(0.975)\n",
    "print(f'{\"We reject the null hypothesis\" if np.abs(W) > z_quantile else \"We do not reject the null hypothesis\"} with cl of 0.95')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#permutations test\n",
    "B=1000\n",
    "bootstrap =[]\n",
    "for _ in range(B):\n",
    "    sample1 = sample.sample(n1, replace=False)\n",
    "    sample2 = sample.drop(sample1.index)\n",
    "    bootstrap.append(sample1['heartRate'].mean() - sample2['heartRate'].mean())\n",
    "alpha = np.sum(np.array([1 if delta<bootstrap[i] else 0 for i in range(len(bootstrap))]))/B\n",
    "print(f'We got that the expected values difference is the {int((1-alpha)*200)} order statistic out of 200,\\n'\n",
    "      f'for pvalue = {alpha} > 0.05 we do not reject the null hypothesis for cl of 0.95')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.4.a:\n",
    "The medians difference is the MLE of a median of $X_{women} - X_{men}$\n",
    "Note that we assume that the population distribution is Gaussian to a sufficiently accurate approximation, for that some of its parameters are equal, e.g., the mean, median, and mode all have the same value (different definitions, but same value).\n",
    "Since the MLE of the mean difference of women and mens is the sample's mean difference so it is also the MLE of the median, and since the median of each sample is equal to its mean (for large n), the sample's median difference is the MLE of the sample's median."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "median1, median2 = sample[['male', 'heartRate']].groupby('male').quantile()['heartRate']\n",
    "delta = mu1 - mu2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.4.b:\n",
    "Define the 50% quantile (=median) of the general distribution to be M.\n",
    "CI for the M's position would be $CI=[np - z_{1-\\frac{\\alpha}{2}} \\sqrt{np(1-p)}, np - z_{1-\\frac{\\alpha}{2}} \\sqrt{np(1-p)}]$\n",
    "Proof : define $I \\sim Ber(p)$, under the null hypothesis, $p=\\frac{1}{2}$ which means that M is the median,\n",
    "under the null hypothesis if M's the median then for n big enough, the middle position of the ordered sample should be held by M.\n",
    "We want to show that the $P_{H0}(\\frac{n}{2} \\in [np - z_{1-\\frac{\\alpha}{2}} \\sqrt{np(1-p)}, np - z_{1-\\frac{\\alpha}{2}} \\sqrt{np(1-p)}]) \\approx 1-\\alpha$\n",
    "$P_{H0}(\\frac{n}{2} \\in [np - z_{1-\\frac{\\alpha}{2}} \\sqrt{np(1-p)}, np - z_{1-\\frac{\\alpha}{2}} \\sqrt{np(1-p)}])$ =\n",
    "$P_{H0}(\\frac{1}{2} \\in [p - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{p(1-p)}{n}}, p + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{p(1-p)}{n}}])$ =\n",
    "$P_{H0}(\\frac{p-\\frac{1}{2}}{\\sqrt{\\frac{p(1-p)}{n}}} \\in [z_{\\frac{\\alpha}{2}}, z_{1-\\frac{\\alpha}{2}}]) = 1-\\alpha$\n",
    "Since $p = \\overline{I}_n$ from Central Limit Theorem $\\frac{\\overline{I}_n-\\frac{1}{2}}{\\sqrt{\\frac{p(1-p)}{n}}} \\sim N(0,1)$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#using Mann-Whitney here\n",
    "diffs = []\n",
    "for x_women in sample[sample['male']==0]['heartRate']:\n",
    "    for x_men in sample[sample['male']==1]['heartRate']:\n",
    "        diffs.append(x_women - x_men)\n",
    "p = np.sum(np.array([1 if delta>=diffs[i] else 0 for i in range(len(bootstrap))]))/B\n",
    "CI = [int(p*(n1+n2)-z_quantile*np.sqrt(p*(1-p)*(n1+n2))), int(p*(n1+n2)+z_quantile*np.sqrt(p*(1-p)*(n1+n2)))]\n",
    "print(f'The CI for the placement of the median in a sample sized 200 is {CI}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.4.c:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "median_all = df['heartRate'].median()\n",
    "heartRates_sorted = np.sort(df['heartRate'].to_numpy())\n",
    "placements = [i for i in range(len(df['heartRate'])) if heartRates_sorted[i] == 75.0]\n",
    "n_whole = df.shape[0]\n",
    "CI_whole = [int(CI[0]*n_whole/(n1+n2)), int(CI[1]*n_whole/(n1+n2))]\n",
    "print(f'The median based on the whole sample is {median_all}')\n",
    "print(f'The CI for the median position (proportionally to the whole samples size) is {CI_whole}')\n",
    "print(f'{\"The median position is in the samples positions CI\" if heartRates_sorted[CI_whole[0]]<=median_all<=heartRates_sorted[CI_whole[1]] else \"The median position is not in the samples positions CI\"}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.4.d:\n",
    "Permutation test :\n",
    "$$\n",
    "H0 : F_{X_{woman}} = F_{X_{man}} \\\\\n",
    "H1 : F_{X_{women}} \\succ F_{X_{men}} \\\\\n",
    "T.S. : T_0 = M_{women} - M_{men} \\\\\n",
    "R.R. : \\frac{1}{N \\choose n_{woman}} \\sum_{i=1}^{N \\choose n_{woman}}I\\{T_i \\geq T_0\\} < \\alpha\n",
    "$$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bootstrap_median =[]\n",
    "for _ in range(B):\n",
    "    sample1 = sample.sample(n1, replace=False)\n",
    "    sample2 = sample.drop(sample1.index)\n",
    "    bootstrap_median.append(sample1['heartRate'].quantile() - sample2['heartRate'].quantile())\n",
    "alpha = np.sum(np.array([1 if delta<bootstrap_median[i] else 0 for i in range(len(bootstrap_median))]))/B\n",
    "print(f'We got that the expected values difference is the {int((1-alpha)*200)} order statistic out of 200,\\n'\n",
    "      f'for pvalue = {alpha} < 0.05 we reject the null hypothesis for cl of 0.95')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.4.e:\n",
    "t-test :\n",
    "t-test is for expected values (or the difference between them) and not for medians diffrence, hence that we cannot use it.\n",
    "Wald test:\n",
    "We could use Wald test (always as the data is gaussian distributed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#showing that the expected values differences is gaussian distributed\n",
    "sns.histplot(bootstrap, color=\"blue\", bins=17)\n",
    "plt.title(\"Bootstrap Heart rate differences distribution of male and female groups\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wilcoxon rank-sum test :\n",
    "$$\n",
    "H0 : F_{X_{woman}} = F_{X_{man}} \\\\\n",
    "H1 : F_{X_{women}} \\succ F_{X_{men}} \\\\\n",
    "T.S. : T = \\frac{W_S - E[W_S]}{\\sqrt{(Var(W_S))}} \\quad (W_S = \\sum_{i=1}^{n_{women}} S_i ,\\quad E(W_S) = \\frac{n_{women}(N+1)}{2}), \\quad Var(W_S) = \\frac{n_{women}n_{men}(N+1)}{12}) \\\\\n",
    "R.R. : T \\geq z_{\\alpha} \\ or \\ -z_{\\alpha} \\geq T \\quad replaces \\quad\\{s : P(W_S \\geq s) \\leq \\alpha\\}\n",
    "$$\n",
    "we can use normal approximation for $W_S$ since under H0 the T.S. is normally distributed\n",
    "When comparing the results of Wilcoxon test and the test based on Resampling, we get the same results basically (which confirms the hypothesis the Wilcoxon T.S. being asymptotically gaussian distributed)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ties_handling(order , data):\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    for i in range(len(counts)):\n",
    "        start, end = degrees_sum(counts,i)\n",
    "        if end==202:\n",
    "            end=200\n",
    "        if end>200:\n",
    "            return order\n",
    "        avg_order = (order[start]+order[end-1])/2\n",
    "        for j in range(start,end):\n",
    "            order[j] = avg_order\n",
    "\n",
    "def degrees_sum(counts, index):\n",
    "    start = int(np.sum([counts[i] for i in range(index)]))\n",
    "    return start, start+counts[index]\n",
    "\n",
    "sample_order = np.argsort(list(sample['heartRate']))+1\n",
    "order_stats_sorted = ties_handling(list(range(1,sample.shape[0]+1)), heartRates_sorted)\n",
    "sample['orderStats'] = [x for _,x in sorted(zip(sample_order,heartRates_sorted))]\n",
    "\n",
    "#Wilcoxon\n",
    "W_S = sample[['male', 'orderStats']].groupby('male').sum()['orderStats'][0]\n",
    "W_S_E = n1*(n1+n2+1)/2\n",
    "W_S_Var = n1*n2*(n1+n2+1)/12\n",
    "T = (W_S - W_S_E) / np.sqrt(W_S_Var)\n",
    "print(f'Wilcoxon test:\\n'\n",
    "      f'We reject the null hypothesis for cl 0.95 since T = {T:.03} < {-z_quantile:.03} = z ')\n",
    "\n",
    "#Resampeling\n",
    "bootstrap_sum =[]\n",
    "for _ in range(B):\n",
    "    sample1 = sample.sample(n1, replace=False)\n",
    "    bootstrap_sum.append(sample1['orderStats'].sum())\n",
    "alpha = np.sum(np.array([1 if W_S<bootstrap_sum[i] else 0 for i in range(len(bootstrap_sum))]))/B\n",
    "print(f'Approximation using Resampling:\\n'\n",
    "      f'We got that the expected values difference is the {int((1-alpha)*200)} order statistic out of 200,\\n'\n",
    "      f'for pvalue = {alpha} < 0.05 thus we do not reject the null hypothesis for cl of 0.95')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "B.6.\n",
    "There's a slight positive statistics difference between the female heart rates and male heart rates, in all kinds of statistics, specially median-wise and degrees sum wise, we guess that we haven't got to reject the null hypothesis when the T.S. was the averaged difference since when looking at the distributions there's a minor difference between the two categories heart rates mean so it is not significant enough to reject the null hypothesis (usually, permutation test is good for extreme statistics results). Looking at the original distributions, we can see that right tail of women heart rate is much longer than the men one, therefore it is reasonable that we got to reject the null hypothesis at almost any of the permutation tests we've considered."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
